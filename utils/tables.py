import os
from typing import Dict, List, Optional, Set, Tuple

import pandas as pd
from IPython.display import display

from utils import stats, utils


def table_suite(data_df: Dict[str, pd.DataFrame],
                suite: str = 'spec06',
                phase: str = 'one_phase',
                metrics: List[str] = ['ipc_improvement']):
    """Summarize statsitics on a single suite.

    Parameters:
        data_df: A dictionary of prefetchers and their statistics dataframes.
        suite: A list of benchmarks in the suite to include.
        phase: Which phase to consider in each benchmark.
        metrics: A list of metrics to include.

    Returns: None

    TODO: Handle multicore
    """
    data_df_ = {k: v[v.cpu0_trace.isin(utils.suites[suite])] 
                for k, v in data_df.items()}

    for k, v in data_df_.items():
        v = v[v.cpu0_simpoint.isin(v for v in utils.phases[phase].values())]
        v = stats.add_means(v)  # Add mean as an extra trace
        v = v.set_index('run_name')
        print(k)
        display(v[metrics])


def table_metric(data_df: Dict[str, pd.DataFrame],
                 suite: str = 'spec06',
                 phase: str = 'one_phase',
                 metric: str = 'ipc_improvement'):
    """Summarize statsitics on a single metric.

    Parameters:
        data_df: A dictionary of prefetchers and their statistics dataframes.
        suite: A list of benchmarks in the suite to include.
        phase: Which phase to consider in each benchmark.
        metric: A metric to include.

    Returns: None
    """
    data_df_ = {k: v[v.cpu0_trace.isin(utils.suites[suite])].copy() 
                for k, v in data_df.items()}

    for k, v in data_df_.items():
        v = v[v.cpu0_simpoint.isin(v for v in utils.phases[phase].values())]
        v = stats.add_means(v)  # Add mean as an extra trace
        v = v.set_index('run_name')[metric]
        v.name = k
        data_df_[k] = v
        #display(v)
    
    metric_df = pd.concat(data_df_.values(), axis=1)
    display(metric_df)


def table_metric_all(data_df: Dict[str, pd.DataFrame],
                     suites: List[Tuple[str, str]] = [('spec06', 'one_phase')],
                     metric: str = 'ipc_improvement'):
    """Summarize statistics on a single metric, across multiple
    suites. The mean is weighted evenly per-benchmark.
    """

    # Gather benchmarks and phases for all traces in all provided suites-phases.
    benchmarks = {}
    for s, p in suites:
        for b in utils.suites[s]:
            benchmarks[b] = utils.phases[p][b]

    # benchmarks, phases = list(benchmarks.keys()), list(benchmarks.values())

    # print(benchmarks)
    # print(phases)
    data_df_ = {k: v[v.cpu0_trace.isin(benchmarks.keys())].copy() 
                for k, v in data_df.items()}
    
    for k, v in data_df_.items():
        v['expected_phase'] = v.cpu0_trace.map(benchmarks)
        v = v[v.cpu0_simpoint == v.expected_phase]
        v = v.drop(columns=['expected_phase'])
        #v = v[v.cpu0_simpoint == benchmarks[v.cpu0_trace]]

        # Add mean over *all* traces
        v = stats.add_means(v) 

        # Add mean over each suite's traces
        for s, p in suites:
            v_suite = v[v.cpu0_trace.isin(utils.suites[s])]
            v_suite = stats.add_means(v_suite)  # Add suite's mean as an extra trace
            v_suite = v_suite[v_suite.run_name == 'mean']
            v_suite.run_name[:] = f'{s} mean'
            v = pd.concat([v, v_suite])

        # Only display means
        v = v[v.run_name.str.contains('mean')]
        v = v.set_index('run_name')[metric]
        v.name = k
        data_df_[k] = v

    metric_df = pd.concat(data_df_.values(), axis=1).T
    display(metric_df)



def table_everything(data_df: Dict[str, pd.DataFrame],
                     suites: List[Tuple[str, str]] = [('spec06', 'one_phase')],
                     metrics: List[str] = ['ipc_improvement']):
    """Summarize statsitics on multiple suites.

    Parameters:
        data_df: A dict of prefetchers and their statistics dataframes.
        suites: A dict of suite names and their lists of benchmarks.
        metrics: A list of metrics to include.

    Returns: None
    """
    # for suite, phase in suites:
    #     print(f'=== {suite} {phase} ===')
    #     table_suite(data_df, suite, phase, metrics)
    for suite, phase in suites:
        print(f'=== {suite} {phase} ===')
        for metric in metrics:
            print(metric)
            table_metric(data_df, suite, phase, metric)


def load_stats_csv(base_dir: str,
                   stats_csv: str,
                   prefetchers: List[str],
                   prefetchers_level = 'l2',
                   separate_degrees: bool = False) -> Dict[str, pd.DataFrame]:
    """Load stats for arbitrary prefetchers.

    Parameters:
        stats_csv: Path to the stats .csv file, as generated by the
            evaluate script in Pythia/experiments.
        prefetchers: A list of prefetchers to include
        separate_degrees: If passed, break down the prefetchers by degree.
            For example, ISB_real with degree 2 will become isb_real_2.
        seed: The ChampSim/Pythia seed to use. If not provided, assume
            the stats only consider one seed.

    Returns:
        data_df: A dict of prefetchers and their statistics dataframes.
    """
    stats_csv = os.path.join(base_dir, stats_csv)
    df = utils.read_data_file(stats_csv)
    df.fillna(0, inplace=True)

    data_df = {}
    if prefetchers == []:
        prefetchers = df[f'{prefetchers_level.upper()}_pref'].unique()

    def get_all_pref_key(pf, level):
        if level == 'l1d':
            return (pf, 'no', 'no')
        elif level == 'l2':
            return ('no', pf, 'no')
        else:
            return ('no', 'no', pf)


    if separate_degrees:
        deg_key = f'{prefetchers_level.upper()}_pref_degree'
        for pf in prefetchers:
            df_ = df[df.all_pref == get_all_pref_key(pf, prefetchers_level)]
            for d in df_[deg_key].unique():
                deg_key_mod = d.replace('(', '').replace(')', '')
                deg_key_mod = deg_key_mod.rstrip(',')
                deg_key_mod = f'{pf}_{deg_key_mod}'
                data_df[deg_key_mod] = df_[df_[deg_key] == d]

    else:
        for pf in prefetchers:
            data_df[pf] = (
                df[df.all_pref == get_all_pref_key(pf, prefetchers_level)])

    return data_df


def merge_best_prefetcher(*dfs, metric='ipc', method='max'):
    """Produce a best prefetcher on each trace, by picking the "best"
    among the provided prefetcher dfs on a metric and method (min/max)
    
    Parameters: TODO
    """
    df_all = pd.concat(dfs)
    if method == 'max':
        return (df_all.sort_values(metric, ascending=False) # max
                      .drop_duplicates('full_trace')
                      .sort_values('full_trace', ascending=True))
                      
    else:
        return (df_all.sort_values(metric, ascending=True) # min
                      .drop_duplicates('full_trace') 
                      .sort_values('full_trace', ascending=True))


def load_stats_csv_pythia(base_dir: str,
                          stats_csv: str,
                          feature_sets: List[Set],
                          feature_key = 'pythia_features') -> Dict[str, pd.DataFrame]:
    """Load stats for specific Pythia feature sets.

    Parameters:
        stats_csv: Path to the stats .csv file, as generated by the 
            evaluate script in Pythia/experiments.
        feature_sets: A list of string sets that represent the Pythia
            feature sets to store (e.g. [{'Delta_Path', 'PC_Delta'}]).
        seed: The ChampSim/Pythia seed to use. If not provided, assume
            the stats only consider one seed.

    Returns:
        data_df: A dict of prefetchers and their statistics dataframes.
    """
    stats_csv = os.path.join(base_dir, stats_csv)

    # Convert string/tuple/pd entry to unordered set.
    def value_to_set(value):
        if pd.notna(value):
            return set(eval(value))
        return set()

    # Convert set to string.
    def set_to_string(set):
        return ', '.join(f for f in sorted(set))

    data_df = {}
    df = utils.read_data_file(stats_csv)
    if feature_sets == []:
        feature_sets = df[feature_key].unique()
        feature_sets = [value_to_set(s) for s in feature_sets]
    for feat_set in feature_sets:
        data_df[set_to_string(feat_set)] = (
            df[df[feature_key].apply(value_to_set) == feat_set])
    return data_df


def load_stats_csv_next_line(base_dir: str, 
                             stats_csv: str,
                             offsets: Optional[List[Set]] = None) -> Dict[str, pd.DataFrame]:
    """Load stats for specific fixed-offsets.

    TODO: docstring
    """
    stats_csv = os.path.join(base_dir, stats_csv)

    data_df = {}
    df = utils.read_data_file(stats_csv)
    if offsets is None:
        offsets = df.next_line_offset.unique()
    for offset in offsets:
        data_df[offset] = df[df.next_line_offset == str(offset)]
    return data_df
